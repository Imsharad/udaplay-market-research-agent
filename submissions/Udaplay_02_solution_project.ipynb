{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# UdaPlay Part 2: AI Agent Development\n",
    "\n",
    "In this notebook, we'll build an intelligent agent that combines local knowledge with web search capabilities.\n",
    "\n",
    "## Objectives:\n",
    "1. Implement three tools: `retrieve_game`, `evaluate_retrieval`, and `game_web_search`\n",
    "2. Build a stateful agent that manages conversation and tool usage\n",
    "3. Implement the workflow: RAG â†’ Evaluate â†’ Web Search (if needed)\n",
    "4. Demonstrate the agent with example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install -q chromadb openai python-dotenv requests pydantic pdfplumber\n",
    "\n",
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "# Add the project's lib directory to the Python path\n",
    "sys.path.append('../projects/building-agents/src/project/starter')\n",
    "\n",
    "# Import our custom modules\n",
    "from lib.llm import LLM\n",
    "from lib.agents import Agent, AgentState\n",
    "from lib.tooling import tool, Tool\n",
    "from lib.vector_db import VectorStore\n",
    "from lib.state_machine import StateMachine, Step, EntryPoint, Termination\n",
    "from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure for Vocareum if using voc- keys\n",
    "if os.environ.get(\"OPENAI_API_KEY\", \"\").startswith(\"voc-\"):\n",
    "    print(\"Detected Vocareum OpenAI API key - configuring for Vocareum endpoint\")\n",
    "    os.environ['OPENAI_API_BASE'] = 'https://openai.vocareum.com/v1'\n",
    "\n",
    "# Verify essential API keys\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not found in environment\"\n",
    "assert os.getenv(\"CHROMA_OPENAI_API_KEY\"), \"CHROMA_OPENAI_API_KEY not found in environment\"\n",
    "assert os.getenv(\"TAVILY_API_KEY\"), \"TAVILY_API_KEY not found in environment\"\n",
    "\n",
    "print(\"âœ… Environment variables loaded.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Initialize Vector Store Connection\n",
    "\n",
    "First, we'll connect to the vector store we created in Part 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Connection (using same VocareumVectorStoreManager as Part 1)\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "class VocareumVectorStoreManager:\n",
    "    \"\"\"Same vector store manager as Part 1 to ensure compatibility.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str):\n",
    "        # Use persistent client so data survives between script runs\n",
    "        self.client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        self.embedding_function = self._create_embedding_function(openai_api_key)\n",
    "\n",
    "    def _create_embedding_function(self, api_key: str):\n",
    "        if api_key.startswith(\"voc-\"):\n",
    "            return embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=api_key, api_base=\"https://openai.vocareum.com/v1\"\n",
    "            )\n",
    "        return embedding_functions.OpenAIEmbeddingFunction(api_key=api_key)\n",
    "\n",
    "    def get_store(self, name: str):\n",
    "        try:\n",
    "            return VectorStore(self.client.get_collection(name=name))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "vector_manager = VocareumVectorStoreManager(openai_api_key=os.getenv(\"CHROMA_OPENAI_API_KEY\"))\n",
    "\n",
    "vector_store = vector_manager.get_store(\"udaplay_games\")\n",
    "\n",
    "if vector_store:\n",
    "    test_results = vector_store.get(limit=1)\n",
    "    print(f\"âœ… Connected to vector store. Sample docs: {len(test_results['ids'])}\")\n",
    "else:\n",
    "    print(\"âŒ Could not locate 'udaplay_games' vector store.\")\n",
    "    print(\"Please run Part 1 first!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Implement Agent Tools\n",
    "\n",
    "Now we'll implement the three required tools for our agent.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: retrieve_game - Search the vector database\n",
    "@tool\n",
    "def retrieve_game(query: str, n_results: int = 3) -> Dict:\n",
    "    \"\"\"Search the vector database for game information.\"\"\"\n",
    "    try:\n",
    "        results = vector_store.query(query_texts=[query], n_results=n_results)\n",
    "        formatted_results = []\n",
    "        if results[\"documents\"] and results[\"documents\"][0]:\n",
    "            for doc, distance, metadata in zip(\n",
    "                results[\"documents\"][0],\n",
    "                results[\"distances\"][0],\n",
    "                results[\"metadatas\"][0],\n",
    "            ):\n",
    "                similarity = 1 - distance\n",
    "                formatted_results.append(\n",
    "                    {\n",
    "                        \"name\": metadata[\"name\"],\n",
    "                        \"platform\": metadata[\"platform\"],\n",
    "                        \"genre\": metadata[\"genre\"],\n",
    "                        \"publisher\": metadata[\"publisher\"],\n",
    "                        \"release_year\": metadata[\"release_year\"],\n",
    "                        \"description\": metadata[\"description\"],\n",
    "                        \"similarity_score\": similarity,\n",
    "                    }\n",
    "                )\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": formatted_results,\n",
    "            \"num_results\": len(formatted_results),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Error retrieving game information: {e}\",\n",
    "            \"query\": query,\n",
    "            \"results\": [],\n",
    "        }\n",
    "\n",
    "# Test the tool\n",
    "test_result = retrieve_game(\"Pokemon games\")\n",
    "print(f\"Retrieved {test_result['num_results']} games\")\n",
    "if test_result['results']:\n",
    "    print(f\"First result: {test_result['results'][0]['name']} ({test_result['results'][0]['release_year']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 2: evaluate_retrieval - Evaluate if results are sufficient \n",
    "@tool\n",
    "def evaluate_retrieval(query: str, retrieved_results: str = \"\") -> Dict:\n",
    "    \"\"\"Evaluate the quality of retrieved results using an LLM.\"\"\"\n",
    "    evaluator = LLM(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    \n",
    "    # Try to parse retrieved_results if it's a string\n",
    "    if isinstance(retrieved_results, str):\n",
    "        try:\n",
    "            import json\n",
    "            retrieved_results_dict = json.loads(retrieved_results)\n",
    "            results_text = (\n",
    "                \"\\n\\n\".join(\n",
    "                    [\n",
    "                        f\"Game {i+1}: {r['name']}\\n\"\n",
    "                        f\"Platform: {r['platform']}\\n\"\n",
    "                        f\"Year: {r['release_year']}\\n\"\n",
    "                        f\"Genre: {r['genre']}\\n\"\n",
    "                        f\"Publisher: {r['publisher']}\\n\"\n",
    "                        f\"Description: {r['description']}\\n\"\n",
    "                        f\"Relevance Score: {r['similarity_score']:.3f}\"\n",
    "                        for i, r in enumerate(retrieved_results_dict.get(\"results\", []))\n",
    "                    ]\n",
    "                )\n",
    "                if retrieved_results_dict.get(\"results\")\n",
    "                else \"No results found.\"\n",
    "            )\n",
    "        except:\n",
    "            # If parsing fails, just use the string representation\n",
    "            results_text = retrieved_results\n",
    "    else:\n",
    "        # If it's already a dict, format it normally\n",
    "        results_text = (\n",
    "            \"\\n\\n\".join(\n",
    "                [\n",
    "                    f\"Game {i+1}: {r['name']}\\n\"\n",
    "                    f\"Platform: {r['platform']}\\n\"\n",
    "                    f\"Year: {r['release_year']}\\n\"\n",
    "                    f\"Genre: {r['genre']}\\n\"\n",
    "                    f\"Publisher: {r['publisher']}\\n\"\n",
    "                    f\"Description: {r['description']}\\n\"\n",
    "                    f\"Relevance Score: {r['similarity_score']:.3f}\"\n",
    "                    for i, r in enumerate(retrieved_results.get(\"results\", []))\n",
    "                ]\n",
    "            )\n",
    "            if retrieved_results.get(\"results\")\n",
    "            else \"No results found.\"\n",
    "        )\n",
    "\n",
    "    evaluation_prompt = f\"\"\"\n",
    "Evaluate if the following search results adequately answer the user's query.\n",
    "\n",
    "User Query: \"{query}\"\n",
    "\n",
    "Retrieved Results:\n",
    "{results_text}\n",
    "\n",
    "Please provide:\n",
    "1. A quality score from 0-10 (10 being perfect)\n",
    "2. A brief explanation of your evaluation\n",
    "3. Whether web search is needed (true/false)\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"quality_score\": <number>,\n",
    "    \"explanation\": \"<your evaluation>\",\n",
    "    \"needs_web_search\": <true/false>,\n",
    "    \"missing_information\": \"<what's missing>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = evaluator.invoke(evaluation_prompt)\n",
    "        evaluation = json.loads(response.content)\n",
    "        # For counting results, handle both string and dict cases\n",
    "        if isinstance(retrieved_results, str):\n",
    "            try:\n",
    "                retrieved_results_dict = json.loads(retrieved_results)\n",
    "                num_results = len(retrieved_results_dict.get(\"results\", []))\n",
    "            except:\n",
    "                num_results = 0\n",
    "        else:\n",
    "            num_results = len(retrieved_results.get(\"results\", []))\n",
    "            \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"quality_score\": evaluation[\"quality_score\"],\n",
    "            \"explanation\": evaluation[\"explanation\"],\n",
    "            \"needs_web_search\": evaluation[\"needs_web_search\"],\n",
    "            \"missing_information\": evaluation.get(\"missing_information\", \"\"),\n",
    "            \"num_results_evaluated\": num_results,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"quality_score\": 0,\n",
    "            \"explanation\": f\"Error during evaluation: {e}\",\n",
    "            \"needs_web_search\": True,\n",
    "            \"missing_information\": \"Unable to evaluate results\",\n",
    "        }\n",
    "\n",
    "# Test the evaluation tool\n",
    "eval_result = evaluate_retrieval(\"Pokemon Gold and Silver\", json.dumps(test_result))\n",
    "print(f\"Quality score: {eval_result['quality_score']}/10\")\n",
    "print(f\"Explanation: {eval_result['explanation']}\")\n",
    "print(f\"Needs web search: {eval_result['needs_web_search']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 3: game_web_search - Search the web via Tavily API\n",
    "@tool\n",
    "def game_web_search(query: str) -> Dict:\n",
    "    \"\"\"Perform a web search via Tavily API for additional game info.\"\"\"\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    payload = {\n",
    "        \"api_key\": tavily_api_key,\n",
    "        \"query\": f\"{query} video game\",\n",
    "        \"search_depth\": \"advanced\",\n",
    "        \"include_answer\": True,\n",
    "        \"include_raw_content\": False,\n",
    "        \"max_results\": 5,\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"title\": r.get(\"title\", \"\"),\n",
    "                \"url\": r.get(\"url\", \"\"),\n",
    "                \"snippet\": r.get(\"content\", \"\"),\n",
    "                \"score\": r.get(\"score\", 0),\n",
    "            }\n",
    "            for r in data.get(\"results\", [])\n",
    "        ]\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": data.get(\"answer\", \"\"),\n",
    "            \"results\": formatted_results,\n",
    "            \"num_results\": len(formatted_results),\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": f\"Web search error: {e}\", \"query\": query, \"results\": []}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {e}\", \"query\": query, \"results\": []}\n",
    "\n",
    "# Test the web search tool\n",
    "web_result = game_web_search(\"Pokemon Gold Silver release date\")\n",
    "print(f\"Web search found {web_result['num_results']} results\")\n",
    "if 'answer' in web_result and web_result['answer']:\n",
    "    print(f\"Quick answer: {web_result['answer'][:100]}...\")\n",
    "print(\"âœ… All three tools implemented and tested!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Build the UdaPlay Agent\n",
    "\n",
    "Now we'll create our stateful agent that combines all three tools in the proper workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Agent Definition\n",
    "class UdaPlayAgent(Agent):\n",
    "    \"\"\"Agent that follows RAG â†’ Evaluate â†’ Web Search workflow.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\", temperature: float = 0.7):\n",
    "        instructions = (\n",
    "            \"You are UdaPlay, an AI research assistant specializing in video game information.\\n\\n\"\n",
    "            \"Workflow:\\n\"\n",
    "            \"1. Use retrieve_game to search the internal database.\\n\"\n",
    "            \"2. Use evaluate_retrieval to assess result quality.\\n\"\n",
    "            \"3. If results are insufficient, use game_web_search.\\n\"\n",
    "            \"4. Return comprehensive, cited answers.\\n\\n\"\n",
    "            \"Answering Guidelines:\\n\"\n",
    "            \"- Always cite sources.\\n\"\n",
    "            \"- Provide specific game details (platform, year, publisher, etc.).\\n\"\n",
    "            \"- If sources conflict, mention both and explain.\\n\"\n",
    "            \"- Maintain conversation context across queries.\"\n",
    "        )\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            instructions=instructions,\n",
    "            tools=[retrieve_game, evaluate_retrieval, game_web_search],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "    def invoke(self, query: str, session_id: Optional[str] = None):\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Processing query: '{query}'\")\n",
    "        print(\"=\" * 60)\n",
    "        result = super().invoke(query, session_id)\n",
    "        final_state = result.get_final_state()\n",
    "        if final_state and \"total_tokens\" in final_state:\n",
    "            print(f\"ðŸ’¬ Total tokens used: {final_state['total_tokens']}\")\n",
    "        return result\n",
    "\n",
    "# Create the agent\n",
    "agent = UdaPlayAgent()\n",
    "print(\"âœ… UdaPlay Agent created successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Demonstrate the Agent\n",
    "\n",
    "Let's test our agent with various queries to show the RAG â†’ Evaluate â†’ Web Search workflow in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display agent responses\n",
    "def display_response(run_result):\n",
    "    final_state = run_result.get_final_state()\n",
    "    if final_state and \"messages\" in final_state:\n",
    "        for msg in reversed(final_state[\"messages\"]):\n",
    "            if getattr(msg, \"content\", None) and not hasattr(msg, \"tool_call_id\"):\n",
    "                print(\"\\nðŸ¤– Agent Response:\\n\" + msg.content)\n",
    "                break\n",
    "\n",
    "# Run demonstration queries\n",
    "def run_demo_queries():\n",
    "    session_id = \"demo_session\"\n",
    "    \n",
    "    queries = [\n",
    "        \"When was PokÃ©mon Gold and Silver released?\",\n",
    "        \"Which one was the first 3D platformer Mario game?\", \n",
    "        \"Was Mortal Kombat X released for PlayStation 5?\",\n",
    "        \"What other Pokemon games were released around the same time?\",\n",
    "    ]\n",
    "    \n",
    "    total_tokens = 0\n",
    "    \n",
    "    for q in queries:\n",
    "        res = agent.invoke(q, session_id=session_id)\n",
    "        display_response(res)\n",
    "        \n",
    "        final_state = res.get_final_state()\n",
    "        if final_state and \"total_tokens\" in final_state:\n",
    "            total_tokens += final_state['total_tokens']\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ DEMO COMPLETE\")\n",
    "    print(f\"Total queries processed: {len(queries)}\")\n",
    "    print(f\"Total tokens used: {total_tokens:,}\")\n",
    "    \n",
    "# Run the demonstration\n",
    "run_demo_queries()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Successfully implemented all requirements:**\n",
    "\n",
    "1. **Three Required Tools:**\n",
    "   - `retrieve_game`: Searches the ChromaDB vector store for game information\n",
    "   - `evaluate_retrieval`: Uses LLM to assess result quality and decide if web search is needed\n",
    "   - `game_web_search`: Performs web search via Tavily API for additional information\n",
    "\n",
    "2. **Stateful Agent:**\n",
    "   - Maintains conversation context across queries\n",
    "   - Follows the proper workflow: RAG â†’ Evaluate â†’ Web Search\n",
    "   - Provides comprehensive, cited responses\n",
    "\n",
    "3. **Demonstration:**\n",
    "   - Processed 4 example queries successfully\n",
    "   - Shows proper tool usage and workflow\n",
    "   - Maintains conversation state between queries\n",
    "\n",
    "The agent successfully demonstrates the intelligent workflow where it first searches local knowledge, evaluates the results, and only performs web search when needed. All responses include proper citations and specific game details.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
